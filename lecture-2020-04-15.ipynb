{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability of Supervised Learning Models\n",
    "\n",
    "**BMI 773 Clinical Research Informatics**\n",
    "\n",
    "*Yuriy Sverchkov*\n",
    "\n",
    "*April 15, 2020*\n",
    "\n",
    "[![xkcd: AI hiring algorithm](images/ai_hiring_algorithm.png)](https://xkcd.com/2237/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "* Z. C. Lipton, “The mythos of model interpretability,” Commun. ACM, vol. 61, no. 10, pp. 36–43, Sep. 2018, [doi: 10.1145/3233231](https://doi.org/10.1145/3233231). [arXiv](https://arxiv.org/pdf/1606.03490v2)\n",
    "* Caruana paper\n",
    "* LIME\n",
    "* GradCAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review (potential readings)\n",
    "\n",
    "* http://philsci-archive.pitt.edu/16734/1/preprint.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we want to interpret models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Trust__: having an interpretation along with a prediction can bring a practitioner to agree with a model.\n",
    "* __Causality__: understanding the associations driving model decisions can help uncover underlying mechanisms.\n",
    "* __Transferability__: understanding how a model makes decisions informs about how it will perform on a different data distribution\n",
    "* __Informativeness__: ppointing out evidence to support a decision (decision support systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes a model interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Simulatability__: Can a person can look at the description of the model and figure out what the model's prediction about a given case would be?\n",
    "* __Decomposability__: Is the model's decision made up of semantically meaningful components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What constitutes an interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model-agnostic vs Model-based\n",
    "* Global vs Local\n",
    "* Feature importances - Matt Churpek mentioned\n",
    "* Feature influences\n",
    "* Models that are interpretable by design\n",
    "    * Logistic regression\n",
    "    * Decision trees\n",
    "    * Rule sets\n",
    "    * Additive model (Caruana)\n",
    "* Model-based post-hoc interpretation\n",
    "    * GradCAM and variants - Matt Churpek mentioned\n",
    "    * Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models that are interpretable by design\n",
    "\n",
    "* Rule-based models\n",
    "* Decision trees\n",
    "* Linear and logistic regression\n",
    "* Additive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting linear regression coefficients\n",
    "\n",
    "__Model:__\n",
    "$y = \\beta_0 + \\sum_{i=1}^d x_i \\beta_i$\n",
    "\n",
    "__Interpretation:__\n",
    "An increase in the value of feature $i$ by 1 unit corresponds to the incease in the outcome by $\\beta_i$ units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting logistic regression coefficients\n",
    "\n",
    "__Model:__\n",
    "$$ \\overbrace{ \\log \\left( \\frac{ P(y=1) }{ P(y=0) }  \\right) }^\\text{log odds} = \\beta_0 + \\sum_{i=1}^d x_i \\beta_i $$\n",
    "\n",
    "__Interpretation:__ An (additive) increase in the value of feature $i$ by 1 unit corresponds to the increase in the odds of the outcome by a (multiplicative) factor of $\\beta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models that generate explanations along with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Image captioning\n",
    "* Attention-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc model-aware interpretation\n",
    "\n",
    "* __Post-hoc__ - the interpretation is not built into the predictive model.\n",
    "* __Model-aware__ - the interpretation exploits knowledge about the model's internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances in random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc model agnostic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feature importance vs model translation](images/akshay-slide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliciting feature importances from black-box models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning high-fidelity mimic models\n",
    "Trepan, LIME, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}