{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (part 2)\n",
    "\n",
    "**BMI 773 Clinical Research Informatics**\n",
    "\n",
    "*Yuriy Sverchkov*\n",
    "\n",
    "*March 25, 2020*\n",
    "\n",
    "[![xkcd: Machine Learning](images/xkcd_machine_learning.png)](https://xkcd.com/1838/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lecture goals\n",
    "\n",
    "*stretch goals in italics*\n",
    "\n",
    "* Understand concepts\n",
    "  * perceptrons\n",
    "  * hidden units\n",
    "  * multilayer neural networks\n",
    "  * gradient descent\n",
    "  * backpropagation\n",
    "  * activation functions\n",
    "    * sigmoid, hyperbolic tangent, ReLU\n",
    "  * loss functions\n",
    "    * squared error, cross entropy\n",
    "  * logistic regression as a neural network\n",
    "  * input encodings\n",
    "  * output encodings\n",
    "  * *autoencodings*\n",
    "  * *word embeddings*\n",
    "  * *recurrent neural networks*\n",
    "  * *convolutional networks*\n",
    "  * *decision trees*\n",
    "* Pitfalls of machine learning\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial neural networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Artificial neural networks have seen much success in recent years.\n",
    "\n",
    "[find examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Biological neurons\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/neuron.png\"></td>\n",
    "        <td><img src=\"images/all-or-none_law_en.svg\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Receive inputs from other neurons via dendrites\n",
    "* Activate at a certain stimulus threshold\n",
    "* Signal to other neurons via terminal branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An artificial neural network's \"neuron\"\n",
    "\n",
    "<img src='images/artificial-neuron.svg' />\n",
    "\n",
    "$$\n",
    "\\underbrace{y}_\\text{output} =\n",
    "\\sigma\\left( \\overbrace{\n",
    "w_0 + \\sum_{i=1}^m \\underbrace{x_i}_\\text{inputs} w_i\n",
    "}^z \\right) =\n",
    "\\sigma(\n",
    "\\underbrace{\\langle w_0, w_1, w_2, \\ldots, w_m \\rangle}_{\\mathbf w} \\cdot\n",
    "\\underbrace{\\langle 1, x_1, x_2, \\ldots, x_m \\rangle}_{\\mathbf x}\n",
    ")\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multilayer neural network\n",
    "\n",
    "A typical neural network would have many neurons arranged in many layers\n",
    "\n",
    "<img src='images/lecun-nature-2015-f1c.png' width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression as a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\hat y = \\mathrm{expit}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_d x_d) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* __Input units:__ $\\boldsymbol x = \\langle 1, x_1, x_2, \\ldots, x_d \\rangle$\n",
    "* __Weights:__ $\\boldsymbol \\beta = \\langle \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_d \\rangle$\n",
    "* __Activation function:__ $\\mathrm{expit}(z) = \\frac{1}{1 + e^{-z}}$\n",
    "* __Output units:__ $\\hat y$\n",
    "* No __hidden units__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# insert image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How did we train logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We had a training set $(\\mathbf x^{(1)}, y^{(1)}), \\ldots, (\\mathbf x^{(n)}, y^{(n)})$\n",
    "* Our model was a function $$ f_{LR}(\\boldsymbol \\beta^{(i)}, \\mathbf x^{(i)}) $$ with parameters $\\boldsymbol \\beta$\n",
    "* We had an instance-wise cost function (cross-entropy loss) $$\n",
    "\\mathrm{cost}(y, \\hat y) =\n",
    " \\begin{cases}\n",
    "  -\\log( \\hat y ) & \\text{ if }y=1 \\\\\n",
    "  -\\log( 1-\\hat y ) & \\text{ if }y=0\n",
    " \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training meant finding the parameters that minimize the average cost across all training instances.\n",
    "\n",
    "$$ \\hat{\\boldsymbol \\beta} = \\underset{\\boldsymbol \\beta}{\\arg \\min} \\frac{1}{n} \\sum_{i=1}^n \\mathrm{cost}(y^{(i)}, f_{LR}(\\boldsymbol \\beta, \\mathbf x^{(i)})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set: $(\\mathbf x^{(1)}, \\mathbf y^{(1)}), \\ldots, (\\mathbf x^{(n)}, \\mathbf y^{(n)})$ for supervised learning\n",
    "  * Still need training sets for unsupervised learning $\\mathbf x^{(1)}, \\ldots, \\mathbf x^{(n)}$\n",
    "  * The targets $\\mathbf y$ may also be multidimensional vectors\n",
    "* The full ANN can still be viewed as a function $f_\\mathrm{net}( \\mathbf W, \\mathbf x )$\n",
    "* $\\mathbf W$ represents all the weights in the network - these are the parameters\n",
    "* Need to pick a cost function (also *objective function*) - the choice depends on the task and data representation\n",
    "* As with LR, the task is to minimize the average cost:\n",
    "$$ \\hat{\\mathbf W} = \\underset{\\mathbf W}{\\arg \\min} \\frac{1}{n} \\sum_{i=1}^n \\mathrm{cost}(y^{(i)}, f_\\mathrm{net}(\\mathbf W, \\mathbf x^{(i)})) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Todo: Find multi-class classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following four slides are from Mark Craven's Machine learning course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/ann-3-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/ann-3-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/ann-3-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/ann-3-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation and gradient descent\n",
    "\n",
    "__Gradient descent__ is a method for finding the minimum of a function with respect to parameters.\n",
    "\n",
    "![Alyhan Tejani: A Brief Introduction To Gradient Descent](gradient_descent_line_graph.gif)\n",
    "\n",
    "__Backpropagation__ is an efficient method for computing the gradient of the cost function with respect to the weights in an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value of multiple layers\n",
    "-a single layer isn't very expressive\n",
    "-show example with 1 input (blood pressure) 1 output (normal/abnormal)\n",
    "-1 layer - not a good fit\n",
    "-2 layers - gets full separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network architecture tailored to the task\n",
    "\n",
    "* Hierarchical networks\n",
    "* Convolutional neural networks\n",
    "* Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hierarchically structured deep network\n",
    "\n",
    "![J. Ma et al., “Using deep learning to model the hierarchical structure and function of a cell,” Nat Methods, vol. 15, no. 4, pp. 290–298, Apr. 2018, doi: 10.1038/nmeth.4627.](images/ma2018title.png)\n",
    "\n",
    "![](images/ma2018fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks\n",
    "\n",
    "![](images/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/convolution.gif\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network\n",
    "\n",
    "![](images/rnn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "[image of a decision tree]\n",
    "\n",
    "What is the optimization problem?\n",
    "\n",
    "A greedy optimisation strategy (greedy strategies are not always perfect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pitfalls\n",
    "\n",
    "- A. Bissoto, M. Fornaciali, E. Valle, and S. Avila, “(De)Constructing Bias on Skin Lesion Datasets,” presented at the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, p. 9.\n",
    "- R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad, “Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’15, Sydney, NSW, Australia, 2015, pp. 1721–1730, doi: 10.1145/2783258.2788613.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane, “Adversarial attacks on medical machine learning,” Science, vol. 363, no. 6433, pp. 1287–1289, Mar. 2019, doi: 10.1126/science.aaw4399.](images/finlayson2019-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/finlayson2019-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/finlayson2019-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Image credits in order of appearance\n",
    "\n",
    "* [XKCD](https://xkcd.com/1838/)\n",
    "* [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:All-or-none_law_en.svg)\n",
    "* [Wikimedia Commons (Prof. Loc Vu-Quoc), CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=72816083)\n",
    "* LeCun et al. Nature 2015\n",
    "* Select slides from Prof. Mark W Craven\n",
    "* [Alyhan Tejani: A Brief Introduction To Gradient Descent](https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/)\n",
    "* CNN: https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/\n",
    "* Convolution gif: http://deeplearning.stanford.edu/\n",
    "* RNN: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg)\n",
    "* Finlayson et al. Science 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
