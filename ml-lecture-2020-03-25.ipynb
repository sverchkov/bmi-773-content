{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (part 2)\n",
    "\n",
    "**BMI 773 Clinical Research Informatics**\n",
    "\n",
    "*Yuriy Sverchkov*\n",
    "\n",
    "*March 25, 2020*\n",
    "\n",
    "[![xkcd: Machine Learning](images/xkcd_machine_learning.png)](https://xkcd.com/1838/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lecture goals\n",
    "\n",
    "*stretch goals in italics*\n",
    "\n",
    "* Understand concepts\n",
    "  * perceptrons\n",
    "  * hidden units\n",
    "  * multilayer neural networks\n",
    "  * gradient descent\n",
    "  * backpropagation\n",
    "  * activation functions\n",
    "    * sigmoid, hyperbolic tangent, ReLU\n",
    "  * loss functions\n",
    "    * squared error, cross entropy\n",
    "  * logistic regression as a neural network\n",
    "  * input encodings\n",
    "  * output encodings\n",
    "  * *autoencodings*\n",
    "  * *word embeddings*\n",
    "  * *recurrent neural networks*\n",
    "  * *convolutional networks*\n",
    "  * *decision trees*\n",
    "* Pitfalls of machine learning\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial neural networks (ANNs) and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Artificial neural networks have seen much success in recent years.\n",
    "\n",
    "[find examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Biological neurons\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/neuron.png\"></td>\n",
    "        <td><img src=\"images/all-or-none_law_en.svg\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Receive inputs from other neurons via dendrites\n",
    "* Activate at a certain stimulus threshold\n",
    "* Signal to other neurons via terminal branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An artificial neural network's \"neuron\"\n",
    "\n",
    "<img src='images/artificial-neuron.svg' />\n",
    "\n",
    "$$\n",
    "\\underbrace{y}_\\text{output} =\n",
    "\\sigma\\left( \\overbrace{\n",
    "w_0 + \\sum_{i=1}^m \\underbrace{x_i}_\\text{inputs} w_i\n",
    "}^z \\right) =\n",
    "\\sigma(\n",
    "\\underbrace{\\langle w_0, w_1, w_2, \\ldots, w_m \\rangle}_{\\mathbf w} \\cdot\n",
    "\\underbrace{\\langle 1, x_1, x_2, \\ldots, x_m \\rangle}_{\\mathbf x}\n",
    ")\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multilayer neural network\n",
    "\n",
    "A typical neural network would have many neurons arranged in many layers\n",
    "\n",
    "<img src='images/lecun-nature-2015-f1c.png' width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression as a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\hat y = \\mathrm{expit}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_d x_d) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='images/lr_net.svg' align='right' />\n",
    "\n",
    "* __Input units:__ $$\\mathbf x = \\langle x_1, x_2, \\ldots, x_d \\rangle$$\n",
    "* __Weights:__ $$\\boldsymbol \\beta = \\langle \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_d \\rangle$$\n",
    "* __Activation function:__ $$\\mathrm{expit}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "* __Output units:__ $$\\hat y$$\n",
    "* No __hidden units__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How did we train logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We had a training set $(\\mathbf x^{(1)}, y^{(1)}), \\ldots, (\\mathbf x^{(n)}, y^{(n)})$\n",
    "* Our model was a function $$ f_{LR}(\\boldsymbol \\beta^{(i)}, \\mathbf x^{(i)}) $$ with parameters $\\boldsymbol \\beta$\n",
    "* We had an instance-wise cost function (cross-entropy loss) $$\n",
    "\\mathrm{cost}(y, \\hat y) =\n",
    " \\begin{cases}\n",
    "  -\\log( \\hat y ) & \\text{ if }y=1 \\\\\n",
    "  -\\log( 1-\\hat y ) & \\text{ if }y=0\n",
    " \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Training meant finding the parameters that minimize the average cost across all training instances.\n",
    "\n",
    "$$ \\hat{\\boldsymbol \\beta} = \\underset{\\boldsymbol \\beta}{\\arg \\min} \\frac{1}{n} \\sum_{i=1}^n \\mathrm{cost}(y^{(i)}, f_{LR}(\\boldsymbol \\beta, \\mathbf x^{(i)})) $$\n",
    "\n",
    "Found the minimum using some numeric optimization software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Training set: $(\\mathbf x^{(1)}, \\mathbf y^{(1)}), \\ldots, (\\mathbf x^{(n)}, \\mathbf y^{(n)})$ for supervised learning\n",
    "  * Still need training sets for unsupervised learning $\\mathbf x^{(1)}, \\ldots, \\mathbf x^{(n)}$\n",
    "  * The targets $\\mathbf y$ may also be multidimensional vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The full ANN can still be viewed as a function $f_\\mathrm{net}( \\mathbf W, \\mathbf x )$\n",
    "* $\\mathbf W$ represents all the weights in the network - these are the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Need to pick a cost function (also *objective function*) - the choice depends on the task and data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* As with LR, the task is to minimize the average cost:\n",
    "$$ \\hat{\\mathbf W} = \\underset{\\mathbf W}{\\arg \\min} \\frac{1}{n} \\sum_{i=1}^n \\mathrm{cost}(y^{(i)}, f_\\mathrm{net}(\\mathbf W, \\mathbf x^{(i)})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following four slides are from Mark Craven's Machine learning course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='images/craven-slides/ann-3-08.png' width='90%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='images/craven-slides/ann-3-09.png' width='90%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/ann-3-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='images/craven-slides/ann-3-11.png' width='90%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing the cost function\n",
    "\n",
    "$$ \\hat{\\mathbf W} = \\underset{\\mathbf W}{\\arg \\min} \\frac{1}{n} \\sum_{i=1}^n \\mathrm{cost}(y^{(i)}, f_\\mathrm{net}(\\mathbf W, \\mathbf x^{(i)})) $$\n",
    "\n",
    "* __Gradient descent__ (and variants of it) is an algorithm for finding the parameter values that minimize the cost function.\n",
    "* __Backpropagation__ is an efficient method for computing the gradient of the cost function with respect to the weights in an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient descent\n",
    "* __Gradient:__ the multidimensional extension of a derivative\n",
    "  * __Derivative:__ the slope of a function (locally) around a point\n",
    "* __Descent:__ Using the gradient of the cost function to figure out how to change the network's weights to decrease the cost\n",
    "\n",
    "![Alyhan Tejani: A Brief Introduction To Gradient Descent](images/gradient_descent_line_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Backpropagation\n",
    "\n",
    "Backpropagation is application of the chain rule from calculus\n",
    "\n",
    "<img src=\"images/toy_net.svg\" />\n",
    "\n",
    "$$ f_\\mathrm{net}(w_1, w_2, x) = f_y( w_2 \\underbrace{ f_h( w_1 x ) }_h ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "  \\frac{\n",
    "    \\partial \\mathrm{cost}(f_\\mathrm{net}(w_1, w_2, x) )\n",
    "  }{\\partial w_2}\n",
    "}_{\\text{change of cost w.r.t. } w_2} =\n",
    "\\overbrace{ \\frac{\\partial \\mathrm{cost} }{ \\partial y } }^{\\text{change of cost w.r.t. } y} \n",
    "\\underbrace{ \\frac{\\partial y }{ \\partial w_2 } }_{\\text{change of } y \\text{ w.r.t. } w_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "  \\frac{\n",
    "    \\partial \\mathrm{cost}(f_\\mathrm{net}(w_1, w_2, x) )\n",
    "  }{\\partial w_1}\n",
    "}_{\\text{change of cost w.r.t. } w_1} =\n",
    "\\overbrace{ \\frac{\\partial \\mathrm{cost} }{ \\partial y } }^{\\text{change of cost w.r.t. } y} \n",
    "\\underbrace{ \\frac{\\partial y }{ \\partial h } }_{\\text{change of } y \\text{ w.r.t. } h}\n",
    "\\overbrace{ \\frac{\\partial h }{ \\partial w_1 } }^{\\text{change of } h \\text{ w.r.t. } w_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Forward pass from inputs to outputs computes values of units\n",
    "2. Backward pass from outputs to inputs computes gradients (derivatives) at those values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network architecture tailored to the task\n",
    "\n",
    "* Hierarchical networks\n",
    "* Convolutional neural networks\n",
    "* Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hierarchically structured deep network\n",
    "\n",
    "![J. Ma et al., “Using deep learning to model the hierarchical structure and function of a cell,” Nat Methods, vol. 15, no. 4, pp. 290–298, Apr. 2018, doi: 10.1038/nmeth.4627.](images/ma2018title.png)\n",
    "\n",
    "![](images/ma2018fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutional neural networks\n",
    "\n",
    "![](images/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/convolution.gif\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recurrent neural network\n",
    "\n",
    "![](images/rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/dt-1-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/craven-slides/dt-1-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spurious correlations\n",
    "\n",
    "![](images/caruana2015-title.png)\n",
    "\n",
    "> The 2nd term in the model, asthma, is the one that caused trouble in the CEHC study in the mid-90’s and preventedclinical trials with the very accurate neural net model.  The GA2M model has found the same pattern discovered backthen: that **having asthma lowers the risk of dying from pneumonia.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bias in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/bissoto2019-title.png)\n",
    "\n",
    "* Patterns in data collection can introduce information about the learning target that is irrelevant to the true task\n",
    "* Example:\n",
    "  * Deep convolutional neural networks trained to classify skin lesions\n",
    "  * What happens when we remove the medically relevant information from the images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/bissoto2019-images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='images/bissoto2019-aucs.png' width=500 align='left' /> The networks still perform well above random chance (AUC=50%) when all information about the lesions is obscured!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img alt='S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane, “Adversarial attacks on medical machine learning,” Science, vol. 363, no. 6433, pp. 1287–1289, Mar. 2019, doi: 10.1126/science.aaw4399.' src='images/finlayson2019-title.png' width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/finlayson2019-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/finlayson2019-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Image credits in order of appearance\n",
    "\n",
    "* [XKCD](https://xkcd.com/1838/)\n",
    "* [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:All-or-none_law_en.svg)\n",
    "* [Wikimedia Commons (Prof. Loc Vu-Quoc), CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=72816083)\n",
    "* LeCun et al. Nature 2015\n",
    "* Select slides from Prof. Mark W Craven\n",
    "* [Alyhan Tejani: A Brief Introduction To Gradient Descent](https://alykhantejani.github.io/a-brief-introduction-to-gradient-descent/)\n",
    "* Ma et al. Nature Methods 2018\n",
    "* CNN: https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/\n",
    "* Convolution gif: http://deeplearning.stanford.edu/\n",
    "* RNN: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg)\n",
    "* Caruana et al. KDD 2015\n",
    "* Bissoto et al. CVPR 2019\n",
    "* Finlayson et al. Science 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
